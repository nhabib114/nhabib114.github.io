---
layout: post
title: Curve-fitting and function approximation
---

A function like F = 1.8C + 32 can be defined by a set of points that are then fitted to a function. In this case the relationship between Celsius and Fahrenheit is linear. 


```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
```

Plot the line F = 1.8C + 32.


```python
fig = plt.figure()
ax = plt.axes()
C = np.linspace(0, 10, 1000)
ax.plot(C, 1.8*C+32)
```




    [<matplotlib.lines.Line2D at 0x7fb9cf445b80>]




    
![png](output_3_1.png)
    


The x-axis is the points generated by np.linspace(), and the y-axis is the mapping of C to F.

The np.linspace() function returns evenly-spaced numbers over an interval. Here we've generated 1000 points from 0 to 10 on the x-axis.


```python
#np.linspace, tf.linspace
tf.linspace() matches the behavior of np.linspace(). (verify)
```


```python
tf.linspace(0, 10, 0)
```




    <tf.Tensor: shape=(0,), dtype=float64, numpy=array([], dtype=float64)>



Like np.linspace, tf.linspace generates evenly-spaced points along one axis over a specified interval. np.linspace(0,10,1000) returns 1000 points between 0 and 10.

With ax.plot, we plot the points on the x-axis against the function 1.8x + 32. 


```python
#suppose we did not know the function that maps F to C. How would we find it? 
```

### Original function

Map a sine function using linspace.


```python
import tensorflow as tf
import math

x = tf.linspace(-math.pi, math.pi, 2000)
y = tf.sin(x)
plt.plot(x, y)
plt.show()
```


    
![png](output_12_0.png)
    


We used matplotlib to plot both the linear function and the sine function. How did we determine what the x and y axes were, and how did we plot them?

We want to use a third-degree polynomial to approximate a sine function. The coefficients of the polynomial are the **weights** of the network we will construct. 

y = a + bx + cx^2 + dx^ 3

### Randomly initialize weights
Initially the four weights we choose for our polynomial approximator are random and completely unconnected to the sine function. 


```python
a = tf.random.uniform(shape=[]) 
b = tf.random.uniform(shape=[])
c = tf.random.uniform(shape=[])
d = tf.random.uniform(shape=[])
```

Calculating the first predicted value is the forward pass. We then calculate the loss using a metric we choose, such as sum of squared errors. We carry out gradient descent to calculate how much to adjust the weights. 

We calculate the sum of squared errors loss by taking the square of the difference between the predicted value and the actual value for each prediction and summing all these errors together. 


```python
learning_rate = 1e-6

for i in range(2000):
    y_pred = a + b*x + c*x**2 + d*x**3 #forward pass
    loss = tf.pow(y_pred - y, 2)
    loss = tf.reduce_sum(loss).numpy() #sum of squared errors
    grad_y_pred = 2.0 * (y_pred - y) 
    
    if i % 100 == 0:
        plt.plot(x, y_pred)
        plt.show()

    grad_a = tf.reduce_sum(grad_y_pred) #gradient descent
    grad_b = tf.reduce_sum(grad_y_pred * x)
    grad_c = tf.reduce_sum(grad_y_pred * x**2)
    grad_d = tf.reduce_sum(grad_y_pred * x**3)
    
    a -= grad_a * learning_rate #update weights
    b -= grad_b * learning_rate
    c -= grad_c * learning_rate
    d -= grad_d * learning_rate

print(f'Result: y = {a.numpy()} + {b.numpy()} x + {c.numpy()} x^2 + {d.numpy()} x^3')
```


    
![png](output_19_0.png)
    



    
![png](output_19_1.png)
    



    
![png](output_19_2.png)
    



    
![png](output_19_3.png)
    



    
![png](output_19_4.png)
    



    
![png](output_19_5.png)
    



    
![png](output_19_6.png)
    



    
![png](output_19_7.png)
    



    
![png](output_19_8.png)
    



    
![png](output_19_9.png)
    



    
![png](output_19_10.png)
    



    
![png](output_19_11.png)
    



    
![png](output_19_12.png)
    



    
![png](output_19_13.png)
    



    
![png](output_19_14.png)
    



    
![png](output_19_15.png)
    



    
![png](output_19_16.png)
    



    
![png](output_19_17.png)
    



    
![png](output_19_18.png)
    



    
![png](output_19_19.png)
    


    Result: y = 0.012272842228412628 + 0.846772313117981 x + -0.0021172689739614725 x^2 + -0.09191244840621948 x^3


We now have four weights that can be used in the polynomial to approximate the sine function. If we had no idea what the sine function was or how to generate it, we could still approximate it with this polynomial.


```python
a = 0.012
b = 0.847
c = -0.002
d = -0.092
```

We update the weights each training cycle using the learning rate, which we determine ahead of time.




